{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PagedPDFSplitter\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "loaders = []\n",
    "\n",
    "\n",
    "\n",
    "def get_pdf_files(filepath):\n",
    "    for filename in os.listdir(filepath):\n",
    "        if filename.endswith('.pdf'):\n",
    "            yield filepath + filename\n",
    "\n",
    "\n",
    "## replace with pdf file paths here\n",
    "path = 'pdfs/'\n",
    "for filename in get_pdf_files(path):\n",
    "    loaders.append(PagedPDFSplitter.from_pdf(filename))\n",
    "\n",
    "\n",
    "## replace with any youtube videos here\n",
    "youtube_files = [\n",
    "    'https://www.youtube.com/watch?v=_hWz0V3Kq64',\n",
    "    'https://www.youtube.com/watch?v=-UO0vKojTn4',\n",
    "    'https://www.youtube.com/watch?v=6DT6y49qV9I',\n",
    "    'https://www.youtube.com/watch?v=oa0hcI4kWqU',\n",
    "    'https://www.youtube.com/watch?v=ljm_wuMOTQM',\n",
    "    'https://www.youtube.com/watch?v=ce0n6WrXREM',\n",
    "]\n",
    "\n",
    "for filename in youtube_files:\n",
    "    loaders.append(YoutubeLoader.from_youtube_url(filename))\n",
    "\n",
    "docsearch = VectorstoreIndexCreator().from_loaders(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, VectorDBQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    ## make this your own key\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    "    ## change this\n",
    "    model='gpt-4'\n",
    ")\n",
    "\n",
    "notes = VectorDBQA.from_chain_type(llm=llm, chain_type=\"stuff\", vectorstore=docsearch.vectorstore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Notes QA System\",\n",
    "        description=\"useful for when you need context from the notes to provide an answer to a given question. Input should be a fully formed question.\",\n",
    "        func=notes.run\n",
    "    )\n",
    "]\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True, max_iterations=5, agent_kwargs={\n",
    "    'prefix':\"\"\"You will be answering as best as possible multiple choice questions about the course material. \n",
    "    If you don't know the answer, and cannot find the answer with the help og the notes, you should say \"I don't know\".\n",
    "    \n",
    "    You have access to the following tools:\n",
    "    \"\"\",\n",
    "    \"input_variables\":[\n",
    "    \"question\",\n",
    "    \"answers\",\n",
    "    \"agent_scratchpad\"\n",
    "    ],\n",
    "    'suffix':\"\"\"\n",
    "\n",
    "    Begin! \n",
    "    Remember, The only action you can use is `Notes QA System`.\n",
    "    Here is the question you need to answer:\n",
    "    {question}\n",
    "    Here are the possible answers:\n",
    "    {answers}\n",
    "\n",
    "    {agent_scratchpad}\n",
    "    \"\"\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Question: When there is a possibility of an extinction burst, you should determine whether:\n",
      "Thought: I need to look at the notes to answer this question\n",
      "Action: Notes QA System\n",
      "Action Input: What should I do when there is a possibility of an extinction burst?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m When there is a possibility of an extinction burst, you should make sure that you have identified the reinforcer(s) and that you can control them. You should also make sure that it is possible to eliminate the reinforcer(s) and that it is safe to use extinction. Additionally, you should ensure that you can apply extinction consistently and that you have procedures in place to promote generalization and maintenance.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the answer to the original question\n",
      "Final Answer: d. A & C\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'d. A & C'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\n",
    "    question=\"\"\"\n",
    "When there is a possibility of an extinction burst, you should determine whether:â€‹\n",
    "    \"\"\",\n",
    "    answers=\"\"\"\n",
    "a.\n",
    "punishment can be implemented for novel behaviors\n",
    "b.\n",
    "the extinction burst is desirable\n",
    "c.\n",
    "the change agent can withhold the reinforcer\n",
    "d.\n",
    "A & C\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anything below this cell does not use an agent, and may provide different results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_choice_prompt_template = \"\"\"\n",
    "Answer the following mutliple choice question by selecting the correct answer from the options below.\n",
    "{question}\n",
    "Use the context to help support your answer.\n",
    "{context}\n",
    "\n",
    "Output the answer as either A, B, C, or D along with the justification for your answer.\n",
    "You MUST include the justification for your answer.\n",
    "\n",
    "If the context does not provide enough information to answer the question, you can say \"I don't know\" and the question will be skipped.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "prompt = PromptTemplate(template=multiple_choice_prompt_template, \n",
    "                        input_variables=['context', 'question']\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_parse_template = \"\"\"\n",
    "Parse the following input into this JSON format: \n",
    "    \\\"answer_a\\\": \\\"A\\\",\n",
    "    \\\"answer_b\\\": \\\"B\\\",\n",
    "    \\\"answer_c\\\": \\\"C\\\",\n",
    "    \\\"answer_d\\\": \\\"D\\\",\n",
    "\n",
    "Ignore when is says `cross out`. That is just a placeholder for the answer. \n",
    "Be sure to wrap the answer in brackets.\n",
    "Here's the input:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "parse_prompt = PromptTemplate(template=prompt_parse_template, input_variables=['input'])\n",
    "from langchain import LLMChain\n",
    "\n",
    "llm = OpenAI(temperature=0.0)\n",
    "\n",
    "parse_chain = LLMChain(llm=llm, prompt=parse_prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A. exclusionary time-out\n",
      "\n",
      "The procedure illustrated in this example is an exclusionary time-out because Johnny is completely removed from the situation and taken to another room (the guest bedroom) as a consequence of his behavior. This is different from a nonexclusionary time-out, where the individual remains in the same room but loses access to positive reinforcement.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "## Change query to your own question\n",
    "query = \"\"\"\n",
    "Johnny hit his sister while they were playing and as a result, his mother made him sit in a chair in the guest bedroom by himself for 5 minutes. What procedure is illustrated in this example?\n",
    "\n",
    "a.\n",
    "exclusionary time-out\n",
    "\n",
    "\n",
    "b.\n",
    "nonexclusionary time-out\n",
    "\n",
    "\n",
    "c.\n",
    "response cost\n",
    "\n",
    "\n",
    "d.\n",
    "extinction\n",
    "\"\"\"\n",
    "question_docs = docsearch.vectorstore.similarity_search(query)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model_name=\"gpt-4\")\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt)\n",
    "\n",
    "\n",
    "print(chain.run({\n",
    "    \"input_documents\": question_docs,\n",
    "    \"question\": query\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langquery-uNSAk5-p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
